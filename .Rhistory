plot(c50.model)
c50.predict = predict(c50.model,testset)
table(c50.predict, testset$churn)
confusionMatrix(table(c50.predict, testset$churn))
ind = cut(1:nrow(churnTrain), breaks=10, labels=F)
ind
accuracies = c()
for (i in 1:10) {
fit = rpart(formula=churn ~., data=churnTrain[ind != i,])
predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")], type="class")
correct_count = sum(predictions == churnTrain[ind == i,c("churn")])
accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)
}
ind = cut(1:nrow(churnTrain), breaks=10, labels=F)
# ind
accuracies = c()
for (i in 1:10) {
fit = rpart(formula=churn ~., data=churnTrain[ind != i,])
predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")], type="class")
correct_count = sum(predictions == churnTrain[ind == i,c("churn")])
accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)
}
# accuracies
mean(accuracies)
# install.packages("caret")
library(caret)
control = trainControl(method="repeatedcv", number=10, repeats=3)
model = train(churn~., data=trainset, method="rpart", trControl=control)
model
predictions = predict(model, testset)
table(predictions,testset$churn)
ind = cut(1:nrow(churnTrain), breaks=10, labels=F)
# ind
accuracies = c()
for (i in 1:10) {
fit = rpart(formula=churn ~., data=churnTrain[ind != i,])
predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")], type="class")
correct_count = sum(predictions == churnTrain[ind == i,c("churn")])
accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)
}
# accuracies
mean(accuracies)
# install.packages("caret")
library(caret)
control = trainControl(method="repeatedcv", number=10, repeats=3)
model = train(churn~., data=trainset, method="rpart", trControl=control)
model
predictions = predict(model, testset)
table(predictions,testset$churn)
library('caret')
importance = varImp(model, scale=FALSE)
importance
plot(importance)
# install.packages("rminer")
library(rminer)
model = fit(churn ~ ., trainset, model="rpart")
# install.packages("rminer")
library(rminer)
model = fit(churn ~ ., trainset, model="rpart")
# install.packages("rminer")
library(rminer)
model = fit(churn ~ ., trainset, model="rpart")
# install.packages("C50")
library(C50)
data(churn)
str(churnTrain)
# 列出所有變數
names(churnTrain)
# 變數比對 Value Matching
names(churnTrain) %in% c("state", "area_code", "account_length")
!names(churnTrain) %in% c("state", "area_code", "account_length")
# 選擇建模變數，排除state, area_code, account_length
variable.list = !names(churnTrain) %in% c('state','area_code','account_length')
churnTrain = churnTrain[,variable.list]
names(churnTrain)
# sample
sample(1:10)
sample(1:10, size = 5)
sample(c(0,1), size= 10, replace = T)  # 取0, 1各10次，取後放回
sample.int(20, 12) # 兩個參數都要放整數，此例為取1:20中的12個不重複樣本
set.seed(2)
# 把資料分成 1:training data 和 2:testing data
ind <- sample(1:2, size=nrow(churnTrain), replace=T, prob=c(0.7, 0.3))
trainset = churnTrain[ind == 1,]
testset = churnTrain[ind == 2,]
table(ind)
table(ind) / nrow(churnTrain)
ind_m <- matrix(table(ind), 1)
ind_m
prop.table(ind_m)
table(sample(x = 1:2,size = 100, replace=T))
set.seed(1)
table(sample(x = 1:2,size = 100, replace=T, prob=c(0.7,0.3)))
a = c(1,2,3,4,5,6,7,8,9)
ind = c(1,0,1,0,1,0,1,0,1)
ind == 1
a[ind == 1]
a[ind == 0]
# install.packages('rpart')
library('rpart')
# 使用rpart(CART)建立決策樹模型
churn.rp <- rpart(churn ~ ., data=trainset)
# summary(churn.rp)
con = rpart.control(cp=0.01)
churn.rp <- rpart(churn ~ ., data=trainset, control = con)
# 畫出決策樹
?plot.rpart
# Set Or Query Graphical Parameters
par(mfrow = c(1,1))
# plot(churn.rp)
# plot(churn.rp, margin=0.1)
# plot(churn.rp, uniform=TRUE, margin=0.1)
plot(churn.rp, uniform=TRUE, branch = 0.6, margin=0.1)
# text(churn.rp)
text(churn.rp, all=TRUE, use.n=TRUE)
printcp(churn.rp)
plotcp(churn.rp)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(churn.rp)
# 找出minimum cross-validation errors
min(churn.rp$cptable[,"xerror"])
which.min(churn.rp$cptable[,"xerror"])
churn.cp = churn.rp$cptable[which.min(churn.rp$cptable[,"xerror"]), "CP"]
# 將churn.cp設為臨界值來修剪樹
prune.tree = prune(churn.rp, cp=churn.cp)
plot(prune.tree, margin=0.1)
text(prune.tree, all=TRUE, use.n=TRUE, cex=0.7)
predictions <- predict(prune.tree, testset,type = "class")
table(testset$churn, predictions)
# install.packages('caret')
# install.packages('e1071')
library('caret')
library('e1071')
confusionMatrix(table(predictions, testset$churn))
?confusionMatrix
# install.packages("party")
library('party')
ctree.model = ctree(churn ~ . , data = trainset)
plot(ctree.model, margin=0.1)
daycharge.model = ctree(churn ~ total_day_charge + international_plan, data = trainset)
plot(daycharge.model)
ctree.predict = predict(ctree.model ,testset)
table(ctree.predict, testset$churn)
confusionMatrix(table(ctree.predict, testset$churn))
# install.packages("C50")
library(C50)
c50.model = C5.0(churn ~., data=trainset)
?C5.0Control
c = C5.0Control(minCases = 20)
c50.model = C5.0(churn ~., data=trainset,control = c)
summary(c50.model)
plot(c50.model)
c50.predict = predict(c50.model,testset)
table(c50.predict, testset$churn)
confusionMatrix(table(c50.predict, testset$churn))
ind = cut(1:nrow(churnTrain), breaks=10, labels=F)
# ind
accuracies = c()
for (i in 1:10) {
fit = rpart(formula=churn ~., data=churnTrain[ind != i,])
predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")], type="class")
correct_count = sum(predictions == churnTrain[ind == i,c("churn")])
accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)
}
# accuracies
mean(accuracies)
# install.packages("caret")
library(caret)
control = trainControl(method="repeatedcv", number=10, repeats=3)
model = train(churn~., data=trainset, method="rpart", trControl=control)
model
predictions = predict(model, testset)
table(predictions,testset$churn)
library('caret')
importance = varImp(model, scale=FALSE)
importance
plot(importance)
# install.packages("rminer")
library(rminer)
model = fit(churn ~ ., trainset, model="rpart")
VariableImportance = Importance(model, trainset, method="sensv")
L = list(runs=1,
sen=t(VariableImportance$imp),
sresponses=VariableImportance$sresponses)
mgraph(L, graph="IMP", leg=names(trainset), col="gray", Grid=10)
# install.packages("ROCR")
library(ROCR)
predictions <-predict(churn.rp, testset, type="prob")
head(predictions)
pred.to.roc <- predictions[, 1]
head(pred.to.roc)
pred.rocr <- prediction(pred.to.roc, testset$churn)
# pred.rocr
perf.rocr <- performance(pred.rocr, measure ="auc", x.measure="cutoff")
# ROC Curvers
perf.tpr.rocr <- performance(pred.rocr, "tpr","fpr")
plot(perf.tpr.rocr,
colorize=T,
main=paste("AUC:",(perf.rocr@y.values)))
# rpart
library('rpart')
churn.rp<-rpart(churn ~., data=trainset)
# ctree
install.packages("party")
library('party')
ctree.model = ctree(churn ~ . , data = trainset)
# C5.0
library(C50)
c50.model = C5.0(churn ~., data=trainset)
rp.predict.prob = predict(churn.rp, testset,type='prob')
c50.predict.prob = predict(c50.model,testset,type='prob')
ctree.predict.prob = sapply(predict(ctree.model ,testset,type='prob'),function(e){unlist(e)[1]})
rp.prediction = prediction(rp.predict.prob[,1],testset$churn)
c50.prediction = prediction(c50.predict.prob[,1],testset$churn)
ctree.prediction = prediction(ctree.predict.prob,testset$churn)
rp.performance = performance(rp.prediction, "tpr","fpr")
c50.performance = performance(c50.prediction, "tpr","fpr")
ctree.performance = performance(ctree.prediction, "tpr","fpr")
plot(rp.performance,col='red')
plot(c50.performance, add=T,col='green')
plot(ctree.performance, add=T,col='blue')
install.packages("party")
install.packages("party")
install.packages("party")
install.packages("party")
# install.packages("C50")
library(C50)
data(churn)
str(churnTrain)
# 列出所有變數
names(churnTrain)
# 變數比對 Value Matching
names(churnTrain) %in% c("state", "area_code", "account_length")
!names(churnTrain) %in% c("state", "area_code", "account_length")
# 選擇建模變數，排除state, area_code, account_length
variable.list = !names(churnTrain) %in% c('state','area_code','account_length')
churnTrain = churnTrain[,variable.list]
names(churnTrain)
# sample
sample(1:10)
sample(1:10, size = 5)
sample(c(0,1), size= 10, replace = T)  # 取0, 1各10次，取後放回
sample.int(20, 12) # 兩個參數都要放整數，此例為取1:20中的12個不重複樣本
set.seed(2)
# 把資料分成 1:training data 和 2:testing data
ind <- sample(1:2, size=nrow(churnTrain), replace=T, prob=c(0.7, 0.3))
trainset = churnTrain[ind == 1,]
testset = churnTrain[ind == 2,]
table(ind)
table(ind) / nrow(churnTrain)
ind_m <- matrix(table(ind), 1)
ind_m
prop.table(ind_m)
table(sample(x = 1:2,size = 100, replace=T))
set.seed(1)
table(sample(x = 1:2,size = 100, replace=T, prob=c(0.7,0.3)))
a = c(1,2,3,4,5,6,7,8,9)
ind = c(1,0,1,0,1,0,1,0,1)
ind == 1
a[ind == 1]
a[ind == 0]
# install.packages('rpart')
library('rpart')
# 使用rpart(CART)建立決策樹模型
churn.rp <- rpart(churn ~ ., data=trainset)
# summary(churn.rp)
con = rpart.control(cp=0.01)
churn.rp <- rpart(churn ~ ., data=trainset, control = con)
# 畫出決策樹
?plot.rpart
# Set Or Query Graphical Parameters
par(mfrow = c(1,1))
# plot(churn.rp)
# plot(churn.rp, margin=0.1)
# plot(churn.rp, uniform=TRUE, margin=0.1)
plot(churn.rp, uniform=TRUE, branch = 0.6, margin=0.1)
# text(churn.rp)
text(churn.rp, all=TRUE, use.n=TRUE)
printcp(churn.rp)
plotcp(churn.rp)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(churn.rp)
# 找出minimum cross-validation errors
min(churn.rp$cptable[,"xerror"])
which.min(churn.rp$cptable[,"xerror"])
churn.cp = churn.rp$cptable[which.min(churn.rp$cptable[,"xerror"]), "CP"]
# 將churn.cp設為臨界值來修剪樹
prune.tree = prune(churn.rp, cp=churn.cp)
plot(prune.tree, margin=0.1)
text(prune.tree, all=TRUE, use.n=TRUE, cex=0.7)
predictions <- predict(prune.tree, testset,type = "class")
table(testset$churn, predictions)
# install.packages('caret')
# install.packages('e1071')
library('caret')
library('e1071')
confusionMatrix(table(predictions, testset$churn))
?confusionMatrix
# install.packages("party")
library('party')
install.packages("party")
# install.packages("party")
library('party')
ctree.model = ctree(churn ~ . , data = trainset)
plot(ctree.model, margin=0.1)
daycharge.model = ctree(churn ~ total_day_charge + international_plan, data = trainset)
plot(daycharge.model)
ctree.predict = predict(ctree.model ,testset)
table(ctree.predict, testset$churn)
confusionMatrix(table(ctree.predict, testset$churn))
# install.packages("C50")
library(C50)
c50.model = C5.0(churn ~., data=trainset)
?C5.0Control
c = C5.0Control(minCases = 20)
c50.model = C5.0(churn ~., data=trainset,control = c)
summary(c50.model)
plot(c50.model)
c50.predict = predict(c50.model,testset)
table(c50.predict, testset$churn)
confusionMatrix(table(c50.predict, testset$churn))
ind = cut(1:nrow(churnTrain), breaks=10, labels=F)
# ind
accuracies = c()
for (i in 1:10) {
fit = rpart(formula=churn ~., data=churnTrain[ind != i,])
predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")], type="class")
correct_count = sum(predictions == churnTrain[ind == i,c("churn")])
accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)
}
# accuracies
mean(accuracies)
# install.packages("caret")
library(caret)
control = trainControl(method="repeatedcv", number=10, repeats=3)
model = train(churn~., data=trainset, method="rpart", trControl=control)
model
predictions = predict(model, testset)
table(predictions,testset$churn)
library('caret')
importance = varImp(model, scale=FALSE)
importance
plot(importance)
# install.packages("rminer")
library(rminer)
model = fit(churn ~ ., trainset, model="rpart")
VariableImportance = Importance(model, trainset, method="sensv")
L = list(runs=1,
sen=t(VariableImportance$imp),
sresponses=VariableImportance$sresponses)
mgraph(L, graph="IMP", leg=names(trainset), col="gray", Grid=10)
# install.packages("ROCR")
library(ROCR)
predictions <-predict(churn.rp, testset, type="prob")
head(predictions)
pred.to.roc <- predictions[, 1]
head(pred.to.roc)
pred.rocr <- prediction(pred.to.roc, testset$churn)
# pred.rocr
perf.rocr <- performance(pred.rocr, measure ="auc", x.measure="cutoff")
# ROC Curvers
perf.tpr.rocr <- performance(pred.rocr, "tpr","fpr")
plot(perf.tpr.rocr,
colorize=T,
main=paste("AUC:",(perf.rocr@y.values)))
# rpart
library('rpart')
churn.rp<-rpart(churn ~., data=trainset)
# ctree
install.packages("party")
library('party')
ctree.model = ctree(churn ~ . , data = trainset)
# C5.0
library(C50)
c50.model = C5.0(churn ~., data=trainset)
rp.predict.prob = predict(churn.rp, testset,type='prob')
c50.predict.prob = predict(c50.model,testset,type='prob')
ctree.predict.prob = sapply(predict(ctree.model ,testset,type='prob'),function(e){unlist(e)[1]})
rp.prediction = prediction(rp.predict.prob[,1],testset$churn)
c50.prediction = prediction(c50.predict.prob[,1],testset$churn)
ctree.prediction = prediction(ctree.predict.prob,testset$churn)
rp.performance = performance(rp.prediction, "tpr","fpr")
c50.performance = performance(c50.prediction, "tpr","fpr")
ctree.performance = performance(ctree.prediction, "tpr","fpr")
plot(rp.performance,col='red')
plot(c50.performance, add=T,col='green')
plot(ctree.performance, add=T,col='blue')
install.packages("party")
min(churn.rp$cptable[,"xerror"])
which.min(churn.rp$cptable[,"xerror"])
churn.cp = churn.rp$cptable[which.min(churn.rp$cptable[,"xerror"]), "CP"]
prune.tree = prune(churn.rp, cp=churn.cp)
plot(prune.tree, margin=0.1)
text(prune.tree, all=TRUE, use.n=TRUE, cex=0.7)
# 找出minimum cross-validation errors
min(churn.rp$cptable[,"xerror"])
which.min(churn.rp$cptable[,"xerror"])
churn.cp = churn.rp$cptable[which.min(churn.rp$cptable[,"xerror"]), "CP"]
# 將churn.cp設為臨界值來修剪樹
prune.tree = prune(churn.rp, cp=churn.cp)
plot(prune.tree, margin=0.1)
text(prune.tree, all=TRUE, use.n=TRUE, cex=0.7)
predictions <- predict(prune.tree, testset,type = "class")
table(testset$churn, predictions)
# install.packages('caret')
# install.packages('e1071')
library('caret')
library('e1071')
confusionMatrix(table(predictions, testset$churn))
?confusionMatrix
min(house.rp$cptable[,"xerror"])
which.min(house.rp$cptable[,"xerror"])
house.rp = house.rp$cptable[which.min(churn.rp$cptable[,"xerror"]), "CP"]
house.rp = house.rp$cptable[which.min(house.rp$cptable[,"xerror"]), "CP"]
house.rp = prune(house.rp, cp=house.rp)
?prune
?prune.tree
?tree
??prune.tree
prune.tree = prune(house.rp, cp=house.rp)
?prune
#(13) 利用rpart套件建立一預測房屋是否為危樓(danger)的決策樹模型，
#     請利用行政區(area), 屋齡(building_age), 房屋總平方米數(building_sqmeter),
#     房屋類型(building_type)及每平方米價格(price_per_sqmeter)
#     5個變數作為解釋變數放入模型當中建模，並將模型存在house.rp變數中。 [5分]
library(rpart)
# 使用rpart(CART)建立決策樹模型
house.rp <- rpart(danger ~ area + building_age + building_sqmeter +
building_type + price_per_sqmeter,
data = trainset,
method = "class")
#(12) 請將house資料以8:2的比例分為訓練集和測試集，
#     將訓練集資料存在trainset變數中，
#     將測試集資料存在testset變數中。 [5分]
set.seed(1206)
house_sample <- sample(1:2, size=nrow(house), replace=T, prob=c(0.8, 0.2))
trainset = house[house_sample==1,]
testset = house[house_sample==2,]
# trainset 38531 obs.
# testset   9805 obs.
#(13) 利用rpart套件建立一預測房屋是否為危樓(danger)的決策樹模型，
#     請利用行政區(area), 屋齡(building_age), 房屋總平方米數(building_sqmeter),
#     房屋類型(building_type)及每平方米價格(price_per_sqmeter)
#     5個變數作為解釋變數放入模型當中建模，並將模型存在house.rp變數中。 [5分]
library(rpart)
# 使用rpart(CART)建立決策樹模型
house.rp <- rpart(danger ~ area + building_age + building_sqmeter +
building_type + price_per_sqmeter,
data = trainset,
method = "class")
# summary(house.rp)
#(14) 請利用plot()和text()畫出house.rp模型的決策樹 [5分]
plot(house.rp, uniform=TRUE)
text(house.rp)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(house.rp, uniform=TRUE)
#(15) 請問此決策數是否需要進行剪枝(prune)？
#     如需剪枝請將修剪後的模型存回house.rp中。 [5分]
# minimum cross-validation errors
min(house.rp$cptable[,"xerror"])
which.min(house.rp$cptable[,"xerror"])
house.cp = house.rp$cptable[which.min(house.rp$cptable[,"xerror"]), "CP"]
# 將house.cp設為臨界值來修剪樹
house.rp = prune(house.rp, cp=house.cp)
#(15) 請問此決策數是否需要進行剪枝(prune)？
#     如需剪枝請將修剪後的模型存回house.rp中。 [5分]
# minimum cross-validation errors
min(house.rp$cptable[,"xerror"])
which.min(house.rp$cptable[,"xerror"])
house.cp = house.rp$cptable[which.min(house.rp$cptable[,"xerror"]), "CP"]
# 將house.cp設為臨界值來修剪樹
house.rp = prune(house.rp, cp=house.cp)
plot(house.rp, uniform=TRUE)
text(house.rp)
#(15) 請問此決策數是否需要進行剪枝(prune)？
#     如需剪枝請將修剪後的模型存回house.rp中。 [5分]
# minimum cross-validation errors
min(house.rp$cptable[,"xerror"])
which.min(house.rp$cptable[,"xerror"])
house.cp = house.rp$cptable[which.min(house.rp$cptable[,"xerror"]), "CP"]
# 將house.cp設為臨界值來修剪樹
house.rp = prune(house.rp, cp=house.cp)
plot(house.rp, uniform=TRUE)
text(house.rp)
#(15) 請問此決策數是否需要進行剪枝(prune)？
#     如需剪枝請將修剪後的模型存回house.rp中。 [5分]
" 此決策數不需要進行剪枝(prune) "
# minimum cross-validation errors
min(house.rp$cptable[,"xerror"])
which.min(house.rp$cptable[,"xerror"])
house.cp = house.rp$cptable[which.min(house.rp$cptable[,"xerror"]), "CP"]
# 將house.cp設為臨界值來修剪樹
house.rp = prune(house.rp, cp=house.cp)
plot(house.rp, uniform=TRUE)
text(house.rp)
#(15) 請問此決策樹是否需要進行剪枝(prune)？
#     如需剪枝請將修剪後的模型存回house.rp中。 [5分]
" 此決策樹不需要進行剪枝(prune) "
# minimum cross-validation errors
min(house.rp$cptable[,"xerror"])
which.min(house.rp$cptable[,"xerror"])
house.cp = house.rp$cptable[which.min(house.rp$cptable[,"xerror"]), "CP"]
# 將house.cp設為臨界值來修剪樹
house.rp = prune(house.rp, cp=house.cp)
plot(house.rp, uniform=TRUE)
text(house.rp)
