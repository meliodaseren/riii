---
title: "R_0725"
author: "York Lin"
date: "2017年7月25日"
output: html_document
---


## Learning map
- http://scikit-learn.org/stable/_static/ml_map.png

- http://www.r-bloggers.com/whats-the-difference-between-machine-learning-statistics-and-data-mining/

- http://mp.weixin.qq.com/s?__biz=MjM5ODczNTkwMA==&mid=2650107069&idx=1&sn=44a2eab6c4858c56af236749fdd1d784#rd


# Classification
## Decision Tree - using churn data in C50 package
```{R}
# install.packages("C50")
library(C50)

data(churn)
str(churnTrain)

# 列出所有變數
names(churnTrain)

# 變數比對 Value Matching
names(churnTrain) %in% c("state", "area_code", "account_length")
!names(churnTrain) %in% c("state", "area_code", "account_length")

# 選擇建模變數，排除state, area_code, account_length
variable.list = !names(churnTrain) %in% c('state','area_code','account_length')
churnTrain = churnTrain[,variable.list]
names(churnTrain)

# sample
sample(1:10)
sample(1:10, size = 5)
sample(c(0,1), size= 10, replace = T)  # 取0, 1各10次，取後放回
sample.int(20, 12) # 兩個參數都要放整數，此例為取1:20中的12個不重複樣本
```

通常產生亂數序列希望是不會重復的。實際上，R 在現在操作視窗下，第一次產生時亂數時，從當下時間 (current time)，生成一個種子 (seed) 出發，不斷迭代更新產生隨機均等分配亂數 (uniform random number)，所以不同時間下執行 R，啟用不同的種子，隨後內部的隨機種子就已經改變了。
模擬亂數是不會重復的，有時我們需要模擬結果是可重復的亂數序列，此時需要用函式 set.seed()，在每次產生偽隨機亂數之前，把種子設定種子為某一特定正整數即可。

```{R}
set.seed(2)
# 把資料分成 1:training data 和 2:testing data
ind <- sample(1:2, size=nrow(churnTrain), replace=T, prob=c(0.7, 0.3))
trainset = churnTrain[ind == 1,]
testset = churnTrain[ind == 2,]
```

```{R}
table(ind)
table(ind) / nrow(churnTrain)

ind_m <- matrix(table(ind), 1)
ind_m
prop.table(ind_m)

table(sample(x = 1:2,size = 100, replace=T))

set.seed(1)
table(sample(x = 1:2,size = 100, replace=T, prob=c(0.7,0.3)))

a = c(1,2,3,4,5,6,7,8,9)
ind = c(1,0,1,0,1,0,1,0,1)
ind == 1
a[ind == 1]
a[ind == 0]
```


## rpart(CART)
https://c3h3notes.wordpress.com/2010/10/25/r%E4%B8%8A%E7%9A%84cart-package-rpart-%E5%8F%83%E6%95%B8%E7%AF%87/

```{R}
# install.packages('rpart')
library('rpart')

<<<<<<< HEAD:20170725.Rmd
# 使用rpart(CART)建立決策樹模型
churn.rp <- rpart(churn ~ ., data=trainset)
# summary(churn.rp)
=======
churn.rp<-rpart(churn ~ ., data=trainset)
#churn.rp<-rpart(churn ~ total_day_charge + #international_plan, data=trainset)
churn.rp
summary(churn.rp)
>>>>>>> 818279985187caa7316f2cb18e17e100df2164aa:history/20170725.Rmd

con = rpart.control(cp=0.01)  # cp 成本複雜度
churn.rp <- rpart(churn ~ ., data=trainset, control = con)
```

### plot.rpart
http://newtomaso.blogspot.tw/2015/02/par-in-r-rpar.html

```{R}
?plot.rpart
<<<<<<< HEAD:20170725.Rmd

# Set Or Query Graphical Parameters
par(mfrow = c(1,1))

# plot(churn.rp)
# plot(churn.rp, margin=0.1)
# plot(churn.rp, uniform=TRUE, margin=0.1)
plot(churn.rp, uniform=TRUE, branch = 0.6, margin=0.1)

# text(churn.rp)
text(churn.rp, all=TRUE, use.n=TRUE)
=======
text(churn.rp)
text(churn.rp, all=TRUE, use.n=TRUE, cex=0.7)
>>>>>>> 818279985187caa7316f2cb18e17e100df2164aa:history/20170725.Rmd

printcp(churn.rp)
plotcp(churn.rp)
```

```{R}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(churn.rp)
```

## Prune
Confusion Matrix
https://blog.argcv.com/articles/1036.c
http://www.cnblogs.com/bluepoint2009/archive/2012/09/18/precision-recall-f_measures.html

```{R}
# 找出minimum cross-validation errors
min(churn.rp$cptable[,"xerror"])
which.min(churn.rp$cptable[,"xerror"])
churn.cp = churn.rp$cptable[which.min(churn.rp$cptable[,"xerror"]), "CP"]

# 將churn.cp設為臨界值來修剪樹
prune.tree = prune(churn.rp, cp=churn.cp)

plot(prune.tree, margin=0.1)
text(prune.tree, all=TRUE, use.n=TRUE, cex=0.7)

<<<<<<< HEAD:20170725.Rmd
predictions <- predict(prune.tree, testset, type = "class")
table(testset$churn, predictions)
=======
predictions <-predict(prune.tree, testset,type='class')
table(predictions,testset$churn)
>>>>>>> 818279985187caa7316f2cb18e17e100df2164aa:history/20170725.Rmd

# install.packages('caret')
# install.packages('e1071')
library('caret')
library('e1071')
confusionMatrix(table(predictions, testset$churn))
?confusionMatrix
```


## Ctree
```{R}
# install.packages("party")
library('party')
ctree.model = ctree(churn ~ . , data = trainset)
plot(ctree.model, margin=0.1)

daycharge.model = ctree(churn ~ total_day_charge + international_plan, data = trainset)
plot(daycharge.model)

ctree.predict = predict(ctree.model ,testset)
table(ctree.predict, testset$churn)

confusionMatrix(table(ctree.predict, testset$churn))
```


## C5.0
```{R}
# install.packages("C50")
library(C50)
c50.model = C5.0(churn ~., data=trainset)

?C5.0Control

c = C5.0Control(minCases = 20)
c50.model = C5.0(churn ~., data=trainset,control = c)

summary(c50.model)
plot(c50.model)

c50.predict = predict(c50.model,testset)
table(c50.predict, testset$churn)

confusionMatrix(table(c50.predict, testset$churn))
```


## Estimating model performance with k-fold cross-validation
```{R}
ind = cut(1:nrow(churnTrain), breaks=10, labels=F)
# ind

accuracies = c()
for (i in 1:10) {
  fit = rpart(formula=churn ~., data=churnTrain[ind != i,])
  predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")], type="class")
  correct_count = sum(predictions == churnTrain[ind == i,c("churn")])
  accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)
}
# accuracies
mean(accuracies)
```


## caret cross-validation
```{R}
# install.packages("caret")
library(caret)
control = trainControl(method="repeatedcv", number=10, repeats=3)
model = train(churn~., data=trainset, method="rpart", trControl=control)
model
predictions = predict(model, testset)

table(predictions,testset$churn)
```


## find importance variable
```{R}
library('caret')
importance = varImp(model, scale=FALSE)
importance
plot(importance)
```

```{r}
# install.packages("rminer")
library(rminer)
model = fit(churn ~ ., trainset, model="rpart")
VariableImportance = Importance(model, trainset, method="sensv")
L = list(runs=1,
         sen=t(VariableImportance$imp),
         sresponses=VariableImportance$sresponses)
mgraph(L, graph="IMP", leg=names(trainset), col="gray", Grid=10)
```


## ROC
- https://www.youtube.com/watch?v=OAl6eAyP-yo
- http://www.navan.name/roc/

```{R}
# install.packages("ROCR")
library(ROCR)
predictions <- predict(churn.rp, testset, type="prob")
head(predictions)

pred.to.roc <- predictions[, 1]
head(pred.to.roc)
<<<<<<< HEAD:20170725.Rmd

pred.rocr <- prediction(pred.to.roc, testset$churn)
# pred.rocr
perf.rocr <- performance(pred.rocr, measure ="auc", x.measure="cutoff")

# ROC Curvers
perf.tpr.rocr <- performance(pred.rocr, "tpr","fpr")

plot(perf.tpr.rocr,
     colorize=T,
     main=paste("AUC:", (perf.rocr@y.values)))
=======
pred.rocr<-prediction(pred.to.roc, testset$churn)
pred.rocr
perf.rocr<-performance(pred.rocr, measure ="auc", x.measure="cutoff")
perf.tpr.rocr<-performance(pred.rocr, "tpr","fpr")
plot(perf.tpr.rocr,main=paste("AUC:",(perf.rocr@y.values)))
>>>>>>> 818279985187caa7316f2cb18e17e100df2164aa:history/20170725.Rmd
```


## model comparison
```{R}
# rpart
library('rpart')
churn.rp<-rpart(churn ~., data=trainset)

# ctree
# install.packages("party")
library('party')
ctree.model = ctree(churn ~ . , data = trainset)

# C5.0
library(C50)
c50.model = C5.0(churn ~., data=trainset)

rp.predict.prob = predict(churn.rp, testset,type='prob')
c50.predict.prob = predict(c50.model,testset,type='prob')
ctree.predict.prob = sapply(predict(ctree.model ,testset,type='prob'),function(e){unlist(e)[1]})
rp.prediction = prediction(rp.predict.prob[,1],testset$churn)
c50.prediction = prediction(c50.predict.prob[,1],testset$churn)
ctree.prediction = prediction(ctree.predict.prob,testset$churn)
rp.performance = performance(rp.prediction, "tpr","fpr")
c50.performance = performance(c50.prediction, "tpr","fpr")
ctree.performance = performance(ctree.prediction, "tpr","fpr")
plot(rp.performance,col='red')
plot(c50.performance, add=T,col='green')
plot(ctree.performance, add=T,col='blue')
```
